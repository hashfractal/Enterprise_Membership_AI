{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bba0a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 선형회귀 분석\n",
    "# Y = WX + b\n",
    "# 데이터 전처리중 해야할 것: x에 대한 y의 값이 극단적이어서 보편적이라고 볼 수 없는경우 해당 데이터를 배제하는것이 합리적"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "726ba5e7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 텐서플로우 라이브러리\n",
    "import tensorflow as tf\n",
    "# 넘파이 -> 수학 관련 기능: 배열 연산 쉽게\n",
    "import numpy as np\n",
    "# 랜덤 함수 관련 변수\n",
    "rng = np.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1fd5b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#학습률 -> 낮으면 느리고 높으면 정확도가 떨어짐, 각 프로젝트마다 적절한 값을 위한 연구필요, 보통 기본값은 0.01\n",
    "learning_rate = 0.01\n",
    "#학습 횟수\n",
    "training_steps =1000\n",
    "#출력 단위\n",
    "display_step =50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc11e3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 배열, X가 문제\n",
    "X = np.array([3.3, 4.4, 5.5, 6.71, 6.93, 4.168, 9.779, 6.182, 7.59, 2.167, 7.042,10.791,5.313, 7.997, 5.654,9.27,3.1])\n",
    "# 배열, Y가 답\n",
    "Y = np.array([1.7, 2.76, 2.09, 3.19,1.694, 1.573, 3.366, 2.596, 2.53, 1.221,2.827, 3.464, 1.65, 2.905, 2.42, 2.94, 1.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9c435d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 가중치 \n",
    "W = tf.Variable(rng.rand(), name = \"weight\")\n",
    "# 편향성(외부요인, 편차)\n",
    "b = tf.Variable(rng.rand(), name = \"bias\")\n",
    "\n",
    "# 선형 회귀 분석 -> 일차 방정식 (Y = WX + B) -> Y를 예측\n",
    "def linear_regression(x):\n",
    "    return W * x + b\n",
    "\n",
    "# Y_Predict -> 예측 Y_True -> 답,\n",
    "# reduce_mean 평균 -> reduce -> 차원 제거 -> 넘파이 배열 여러개 -> 함수 여러번 호출 -> 결과 여러개 -> 1차원\n",
    "# square 제곱\n",
    "# Y = Wx + b\n",
    "def mean_square(y_pred, y_true):\n",
    "    return tf.reduce_mean(tf.square(y_pred - y_true)) \n",
    "\n",
    "# 최적화 알고리즘\n",
    "# Stochastic Gradient Descent -> SGD: 확률적 경사 하강법, 기울기가 0에 가까운 근사값을 찾는 알고리즘\n",
    "optimizer = tf.optimizers.SGD(learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc342517",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 최적화 함수\n",
    "def run_optimization():\n",
    "    # GradientTape: 미분 -> 기울기를 얻기 위해\n",
    "    with tf.GradientTape() as g:\n",
    "        # prad -> 넘파이배열\n",
    "        # y 예측 배열 - > 넘파이 배열의 함수연산 결과값도 배열로\n",
    "        pred = linear_regression(X)\n",
    "        # loss: 편차, 제곱의 평균 -> 예측 결과와 답과의 오차\n",
    "        loss = mean_square(pred, Y)\n",
    "        #gradients 기울기 관련 데이터\n",
    "        gradients = g.gradient(loss,[W,b])\n",
    "        # 기울기 관련 데이터를 이용해서 loss가 0에 수렴하도록 W, b를 설정\n",
    "        optimizer.apply_gradients(zip(gradients, [W,b]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2d9c4a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 50, loss: 0.195438. W: 0.333334, b: 0.219563\n",
      "step: 100, loss: 0.186469. W: 0.323982, b: 0.285870\n",
      "step: 150, loss: 0.179436. W: 0.315699, b: 0.344589\n",
      "step: 200, loss: 0.173921. W: 0.308364, b: 0.396589\n",
      "step: 250, loss: 0.169595. W: 0.301869, b: 0.442638\n",
      "step: 300, loss: 0.166203. W: 0.296117, b: 0.483417\n",
      "step: 350, loss: 0.163543. W: 0.291023, b: 0.519529\n",
      "step: 400, loss: 0.161457. W: 0.286512, b: 0.551509\n",
      "step: 450, loss: 0.159821. W: 0.282518, b: 0.579830\n",
      "step: 500, loss: 0.158538. W: 0.278980, b: 0.604910\n",
      "step: 550, loss: 0.157531. W: 0.275848, b: 0.627119\n",
      "step: 600, loss: 0.156742. W: 0.273073, b: 0.646787\n",
      "step: 650, loss: 0.156124. W: 0.270617, b: 0.664204\n",
      "step: 700, loss: 0.155638. W: 0.268441, b: 0.679628\n",
      "step: 750, loss: 0.155258. W: 0.266514, b: 0.693287\n",
      "step: 800, loss: 0.154959. W: 0.264808, b: 0.705383\n",
      "step: 850, loss: 0.154725. W: 0.263297, b: 0.716095\n",
      "step: 900, loss: 0.154542. W: 0.261959, b: 0.725581\n",
      "step: 950, loss: 0.154398. W: 0.260774, b: 0.733981\n",
      "step: 1000, loss: 0.154285. W: 0.259725, b: 0.741421\n"
     ]
    }
   ],
   "source": [
    "for step in range(1, training_steps +1):\n",
    "    run_optimization()\n",
    "    \n",
    "    #step -> 현제 반복횟수\n",
    "    if step % display_step == 0:\n",
    "        pred = linear_regression(X)\n",
    "        loss = mean_square(pred, Y)\n",
    "        print(\"step: %i, loss: %f. W: %f, b: %f\" % (step, loss, W.numpy(), b.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad233db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(X,Y ,'ro', label =\"Original data\")\n",
    "plt.plot(X, np.array(W * X + b ), label ='Fitted.line')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bee6f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
